{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Narrow-sense heritability\n",
    "\n",
    "Narrow sense heritability, $h^2$ is the ratio of variance due to average/additive effects of alleles. Where $Var(A)$ is the variance due to this addative effect and $Var(P)$ is the total phenotypic variance:\n",
    "\n",
    "$$h^2 = \\frac{Var(A)}{Var(P)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing $h^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total variance of the phenotype includes the variance from the additive genetic component $Var(A)$, as well as a components from environemntal and non-additive genetic effects which are collectively $Var(\\neg A)$. \n",
    "\n",
    "$$Var(P) = Var(A) + Var(\\neg A)$$\n",
    "\n",
    "We will model the additive component of the phenotype $A$ by training a linear model that will predict a phenotype value given the input genotype. The model will learn one weight per variant (i.e. SNPs from both haplotypes will be combined into a single feature with a value of 0, 1, or 2). The coefficient of determination for this model is the narrow sense heritability $h^2$, but I'll derive that from the fraction of variance unexplained for clarity.\n",
    "\n",
    "The fraction of variance unexplained (FVU) is the fraction of variance for a target variable in a regression not explained by the model. In this case, the target variable is the phenotype and the model is the linear model of the additive genetic component. For a linear regression, this is equivalent to $ 1 - R^2 $, where $R^2$ is the coefficient of determination for the linear regression model.\n",
    "\n",
    "$$ FVU = \\frac{Var(\\neg A)}{Var(P)} = 1 - R^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can find that $h^2 = R^2$.\n",
    "\n",
    "$$ Var(A) = Var(P) (1 - FVU) = Var(P) [1 - (1 - R^2)] = Var(P) R^2 $$\n",
    "\n",
    "$$ h^2 = \\frac{Var(A)}{Var(P)} = \\frac{Var(P) R^2}{Var(P)} = R^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan is to simulate $n_{train}$ phenotypes per genotype for training and then compute $R^2$ using $n_{test}$ different simulated phenotypes per genotype, but I'm not sure if it matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement estimation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from pheno_sim import PhenoSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_config(heritability=1.0, combine_type=\"AdditiveCombine\"):\n",
    "\t\"\"\"Return linear additive model config with specified heritability\n",
    "\tand haplotype combine node type.\n",
    "\t\"\"\"\n",
    "\n",
    "\treturn {\n",
    "\t\t\"input\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"file\": \"1000_genomes_data/ALL.chr19.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\",\n",
    "\t\t\t\t\"file_format\": \"vcf\",\n",
    "\t\t\t\t\"reference_genome\": \"GRCh37\",\n",
    "\t\t\t\t\"force_bgz\": True,\n",
    "\t\t\t\t\"input_nodes\": [\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"alias\": \"rare_variants\",\n",
    "\t\t\t\t\t\t\"type\": \"SNP\",\n",
    "\t\t\t\t\t\t\"chr\": \"19\",\n",
    "\t\t\t\t\t\t\"pos\": [13397324, 52901080, 16023111]\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"alias\": \"common_variants\",\n",
    "\t\t\t\t\t\t\"type\": \"SNP\",\n",
    "\t\t\t\t\t\t\"chr\": \"19\",\n",
    "\t\t\t\t\t\t\"pos\": [55555845, 55261043, 45857820, 55442280, 39369369]\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t]\n",
    "\t\t\t}\n",
    "\t\t],\n",
    "\t\t\"simulation_steps\": [\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"RandomConstant\",\n",
    "\t\t\t\t\"alias\": \"rare_variants_betas\",\n",
    "\t\t\t\t\"input_match_size\": \"rare_variants\",\n",
    "\t\t\t\t\"dist_name\": \"normal\",\n",
    "\t\t\t\t\"dist_kwargs\": {\n",
    "\t\t\t\t\t\"loc\": 0.0,\n",
    "\t\t\t\t\t\"scale\": 0.5\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"by_feat\": True\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"RandomConstant\",\n",
    "\t\t\t\t\"alias\": \"common_variants_betas\",\n",
    "\t\t\t\t\"input_match_size\": \"common_variants\",\n",
    "\t\t\t\t\"dist_name\": \"normal\",\n",
    "\t\t\t\t\"dist_kwargs\": {\n",
    "\t\t\t\t\t\"loc\": 0.0,\n",
    "\t\t\t\t\t\"scale\": 0.2\n",
    "\t\t\t\t},\n",
    "\t\t\t\t\"by_feat\": True\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"Product\",\n",
    "\t\t\t\t\"alias\": \"rare_variant_effects\",\n",
    "\t\t\t\t\"input_aliases\": [\n",
    "\t\t\t\t\t\"rare_variants_betas\", \"rare_variants\"\n",
    "\t\t\t\t]\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"Product\",\n",
    "\t\t\t\t\"alias\": \"common_variant_effects\",\n",
    "\t\t\t\t\"input_aliases\": [\n",
    "\t\t\t\t\t\"common_variants_betas\", \"common_variants\"\n",
    "\t\t\t\t]\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"Concatenate\",\n",
    "\t\t\t\t\"alias\": \"variant_effects\",\n",
    "\t\t\t\t\"input_aliases\": [\n",
    "\t\t\t\t\t\"rare_variant_effects\", \"common_variant_effects\"\n",
    "\t\t\t\t],\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": combine_type,\n",
    "\t\t\t\t\"alias\": \"combined_variant_effects\",\n",
    "\t\t\t\t\"input_alias\": \"variant_effects\"\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"SumReduce\",\n",
    "\t\t\t\t\"alias\": \"no_noise_phenotype\",\n",
    "\t\t\t\t\"input_alias\": \"combined_variant_effects\"\n",
    "\t\t\t},\n",
    "\t\t\t{\n",
    "\t\t\t\t\"type\": \"Heritability\",\n",
    "\t\t\t\t\"alias\": \"phenotype\",\n",
    "\t\t\t\t\"input_alias\": \"no_noise_phenotype\",\n",
    "\t\t\t\t\"heritability\": heritability\n",
    "\t\t\t}\n",
    "\t\t]\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: No SLF4J providers were found.\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.\n",
      "SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.\n",
      "SLF4J: Ignoring binding found at [jar:file:/Users/ross/miniforge3/envs/pheno_sim/lib/python3.10/site-packages/pyspark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.2\n",
      "SparkUI available at http://rosss-mbp.lan:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.118-a4ca239602bb\n",
      "LOGGING: writing to /Users/ross/Desktop/gwas/CITRUS/doc/example_nbs/hail-20230809-1305-0.2.118-a4ca239602bb.log\n",
      "2023-08-09 13:05:10.534 Hail: INFO: scanning VCF for sortedness...\n",
      "2023-08-09 13:05:34.830 Hail: INFO: Coerced prefix-sorted VCF, requiring additional sorting within data partitions on each query.\n",
      "2023-08-09 13:07:52.092 Hail: INFO: wrote matrix table with 8 rows and 2504 columns in 3 partitions to /tmp/persist_MatrixTableQDGi0lreRQ\n"
     ]
    }
   ],
   "source": [
    "# Get input data\n",
    "sim = PhenoSimulation(gen_config())\n",
    "input_vals = sim.run_input_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from sklearn import model_selection\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_vals_dict(vals_dict, n_samples=1.0):\n",
    "\t\"\"\"Subsample individuals in vals dict.\n",
    "\t\n",
    "\tIf n_samples is less than 1, will use that fraction of samples. If\n",
    "\tn_samples is greater than 1, will use that number of samples. If \n",
    "\tn_samples is 1 (default), will return the same vals dict.\n",
    "\t\n",
    "\tArgs:\n",
    "\t\tvals (dict): Dictionary of values.\n",
    "\t\tn_samples (default 1.0): The fraction or number of samples to\n",
    "\t\t\tuse when estimating heritability. If value is less than or\n",
    "\t\t\tequal to 1, will use that fraction of samples. If value is\n",
    "\t\t\tgreater than 1, will use that number of samples. Samples\n",
    "\t\t\twill be randomly selected from the input samples.\n",
    "\t\"\"\"\n",
    "\tvals_dict = copy.deepcopy(vals_dict)\n",
    "\t\n",
    "\t# Get number of sample in input.\n",
    "\tinput_item = vals_dict[list(vals_dict.keys())[0]]\n",
    "\tif isinstance(input_item, tuple):\n",
    "\t\tn_input_samples = input_item[0].shape[-1]\n",
    "\telse:\n",
    "\t\tn_input_samples = input_item.shape[-1]\n",
    "\n",
    "\t# Shuffle and possibly subsample input data\n",
    "\tif n_samples > 1:\n",
    "\t\tselected_idx = np.random.choice(\n",
    "\t\t\tn_input_samples,\n",
    "\t\t\tn_samples,\n",
    "\t\t\treplace=False\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tfor key, val in vals_dict.items():\n",
    "\t\t\tif isinstance(val, tuple):\n",
    "\t\t\t\tvals_dict[key] = (\n",
    "\t\t\t\t\tval[0][..., selected_idx], val[1][..., selected_idx]\n",
    "\t\t\t\t)\n",
    "\t\t\telse:\n",
    "\t\t\t\tvals_dict[key] = val[..., selected_idx]\n",
    "\telif n_samples < 1:\n",
    "\t\tselected_idx = np.random.choice(\n",
    "\t\t\tn_input_samples,\n",
    "\t\t\tint(n_samples * n_input_samples),\n",
    "\t\t\treplace=False\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tfor key, val in vals_dict.items():\n",
    "\t\t\tif isinstance(val, tuple):\n",
    "\t\t\t\tvals_dict[key] = (\n",
    "\t\t\t\t\tval[0][..., selected_idx], val[1][..., selected_idx]\n",
    "\t\t\t\t)\n",
    "\t\t\telse:\n",
    "\t\t\t\tvals_dict[key] = val[..., selected_idx]\n",
    "\n",
    "\treturn vals_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.45it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      " 40%|████      | 2/5 [00:00<00:01,  2.26it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      " 60%|██████    | 3/5 [00:01<00:01,  1.80it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      " 80%|████████  | 4/5 [00:01<00:00,  2.27it/s][Parallel(n_jobs=-1)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Estimate_narrow_heritability method\n",
    "sim = PhenoSimulation(gen_config(\n",
    "\theritability=0.5,\n",
    "\tcombine_type=\"AdditiveCombine\"\n",
    "))\n",
    "input_vals = input_vals\n",
    "n_samples = 0.1\n",
    "n_replicates = 250\n",
    "n_folds = 5\n",
    "n_repeats = 5\n",
    "statistic_fn = np.mean\n",
    "confidence_level=0.95\n",
    "phenotype_alias = 'phenotype'\n",
    "\n",
    "\n",
    "# Simulate and then get CV scores n_repeats times\n",
    "r2_scores = []\n",
    "\n",
    "for i in trange(n_repeats):\n",
    "\t# Subsample individuals if n_samples is not 1\n",
    "\titer_input_vals = sample_vals_dict(input_vals, n_samples)\n",
    "\n",
    "\t# Simulate phenotypes to create labels, tracking genotype\n",
    "\tpheno_vals = []\n",
    "\tgeno_idx = []\n",
    "\n",
    "\tfor i in range(n_replicates):\n",
    "\t\tgen_phenos = sim.run_simulation_steps(\n",
    "\t\t\tcopy.deepcopy(iter_input_vals)\n",
    "\t\t)[phenotype_alias]\n",
    "\n",
    "\t\tpheno_vals.extend(gen_phenos)\n",
    "\t\tgeno_idx.extend(list(range(len(gen_phenos))))\n",
    "\n",
    "\t# Convert input values to a dataframe and stack to match n_replicates\n",
    "\tinput_df = sim.vals_dict_to_dataframe(iter_input_vals)\n",
    "\tinput_df = pd.concat([input_df] * n_replicates)\n",
    "\n",
    "\t# Create model and cross validation object\n",
    "\tlinear_reg = linear_model.LinearRegression(n_jobs=-1)\n",
    "\tstrat_group_kfold = model_selection.GroupKFold(\n",
    "\t\tn_splits=n_folds\n",
    "\t)\n",
    "\n",
    "\t# Get R^2 scores for each fold\n",
    "\tr2_scores.extend(\n",
    "\t\tmodel_selection.cross_val_score(\n",
    "\t\t\tlinear_reg,\n",
    "\t\t\tX=input_df,\n",
    "\t\t\ty=pheno_vals,\n",
    "\t\t\tgroups=geno_idx,\n",
    "\t\t\tscoring='r2',\n",
    "\t\t\tcv=strat_group_kfold,\n",
    "\t\t\tn_jobs=-1,\n",
    "\t\t\tverbose=1\n",
    "\t\t)\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean R2: 0.483\n",
      "95% CI: [0.462, 0.504]\n"
     ]
    }
   ],
   "source": [
    "# Compute bootstrapped confidence intervals\n",
    "ci = stats.bootstrap(\n",
    "    (r2_scores,), \n",
    "    statistic_fn,\n",
    "    confidence_level=confidence_level,\n",
    "    # num_iterations=1000\n",
    ")\n",
    "\n",
    "median_r2 = statistic_fn(r2_scores)\n",
    "lower_ci = ci.confidence_interval.low\n",
    "upper_ci = ci.confidence_interval.high\n",
    "\n",
    "print(f\"{statistic_fn.__name__.title()} R2: {median_r2:.3f}\")\n",
    "print(f\"95% CI: [{lower_ci:.3f}, {upper_ci:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pheno_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
